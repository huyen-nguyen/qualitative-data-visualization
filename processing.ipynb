{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an environmental studies\n",
      "I\n",
      "tons\n",
      "data\n",
      "environmental issues\n",
      "the results\n",
      "an audience\n",
      "I\n",
      "this class\n",
      "me\n",
      "the skills\n",
      "large pieces\n",
      "data\n",
      "results\n",
      "what\n",
      "the data\n",
      "a certain environmental issue\n",
      "I\n",
      "the ability\n",
      "sense\n",
      "copious amounts\n",
      "data\n",
      "patterns\n",
      "I\n",
      "lecture style teachings\n",
      "a portion\n",
      "class time\n",
      "individual work\n",
      "my best way\n",
      "a topic\n",
      "hour\n",
      "long lectures\n",
      "no time\n",
      "the material\n",
      "me\n",
      "my best work\n",
      "{'gain', 'sit', 'make', 'designate', 'work', 'simplify', 'communicate', 'report', 'learn', 'collect', 'analyze', 'need', 'leave', 'tie', 'produce', 'will', 'can', 'feel', 'provide', 'hope', 'conclude'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import csv\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "text = \"As an environmental studies major, I need to work with tons of data collected on environmental issues and be able to thoroughly communicate the results to an audience. I feel this class will provide me the skills to be able to accurately analyze large pieces of data and be able to produce results that simplify what the data concludes on a certain environmental issue. I hope to gain the ability to make sense of copious amounts of data and sense patterns that can be reported.I feel that lecture style teachings tied with a portion of class time designated to individual work is my best way of learning a topic. Sitting through hour long lectures with no time to work through the material leaves me frustrated and unable to produce my best work.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, \n",
    "#           chunk.root.text, \n",
    "#           chunk.root.dep_,\n",
    "#           chunk.root.head.text\n",
    "         )\n",
    "\n",
    "verbs = set()\n",
    "\n",
    "for token in doc:\n",
    "    if (token.pos_ == \"VERB\"):\n",
    "        verbs.add(token.lemma_)\n",
    "        \n",
    "print(verbs)\n",
    "\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201\n",
      "20201\n",
      "20201\n",
      "20201\n",
      "20201\n",
      "20201\n",
      "20201\n",
      "20201\n",
      "20201\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20194\n",
      "20202\n",
      "20202\n",
      "20202\n",
      "20202\n",
      "20202\n",
      "20202\n",
      "20202\n",
      "20202\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/journal-data.csv\", mode='r', encoding=\"latin-1\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "            print(row[\"YearQuarter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'merge_noun_chunks' already exists in pipeline. Existing names: ['tagger', 'parser', 'ner', 'merge_noun_chunks']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2db9f665132d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmerge_nps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merge_noun_chunks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_nps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I have a blue car\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_component_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE006\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E007] 'merge_noun_chunks' already exists in pipeline. Existing names: ['tagger', 'parser', 'ner', 'merge_noun_chunks']"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
